# Estimated time for
# 25 epochs
# pitzer with 4 GPUS
# train_100 data set       = 2 days, 16 hours and 40 minutes

# 2 epochs
# pitzer with 4 GPUS
# train_100 data set       = about 6 hours

# Old configuration requires Tesla V100-SXM2(32GB) x 16 GPUs It takes about 2 days.
use_amp: true
lm: transformer
lm_conf:
    pos_enc: null
    embed_unit: 128
    att_unit: 512
    head: 8
    unit: 2048
    layer: 16
    dropout_rate: 0.0

# optimization related
grad_clip: 5.0
batch_type: numel
batch_bins: 100000000 # Formerly 500000000
accum_grad: 2
max_epoch: 3 # 25

optim: adam
optim_conf:
   lr: 0.005
scheduler: warmuplr
scheduler_conf:
   warmup_steps: 25000

best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10  # 10 is good.

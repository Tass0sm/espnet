# python3 -m espnet2.bin.tts_train --collect_stats true --write_collected_feats false --use_preprocessor true --token_type phn --token_list dump/token_list/phn_tacotron_g2p_en/tokens.txt --non_linguistic_symbols none --cleaner tacotron --g2p g2p_en --normalize none --pitch_normalize none --energy_normalize none --train_data_path_and_name_and_type dump/raw/train_nodev/text,text,text --train_data_path_and_name_and_type dump/raw/train_nodev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/train_dev/text,text,text --valid_data_path_and_name_and_type dump/raw/train_dev/wav.scp,speech,sound --train_shape_file exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/train.10.scp --valid_shape_file exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/valid.10.scp --output_dir exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/stats.10 --config conf/train_transformer.yaml --feats_extract fbank --feats_extract_conf n_fft=1024 --feats_extract_conf hop_length=256 --feats_extract_conf win_length=null --feats_extract_conf fs=16000 --feats_extract_conf fmin=80 --feats_extract_conf fmax=7600 --feats_extract_conf n_mels=80 --pitch_extract_conf fs=16000 --pitch_extract_conf n_fft=1024 --pitch_extract_conf hop_length=256 --pitch_extract_conf f0max=400 --pitch_extract_conf f0min=80 --energy_extract_conf fs=16000 --energy_extract_conf n_fft=1024 --energy_extract_conf hop_length=256 --energy_extract_conf win_length=null 
# Started at Thu Mar 30 09:46:53 CDT 2023
#
/home/karimimonsefi.1/miniconda3/envs/espnet_env/bin/python3 /home/karimimonsefi.1/espnet/espnet2/bin/tts_train.py --collect_stats true --write_collected_feats false --use_preprocessor true --token_type phn --token_list dump/token_list/phn_tacotron_g2p_en/tokens.txt --non_linguistic_symbols none --cleaner tacotron --g2p g2p_en --normalize none --pitch_normalize none --energy_normalize none --train_data_path_and_name_and_type dump/raw/train_nodev/text,text,text --train_data_path_and_name_and_type dump/raw/train_nodev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/train_dev/text,text,text --valid_data_path_and_name_and_type dump/raw/train_dev/wav.scp,speech,sound --train_shape_file exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/train.10.scp --valid_shape_file exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/valid.10.scp --output_dir exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/stats.10 --config conf/train_transformer.yaml --feats_extract fbank --feats_extract_conf n_fft=1024 --feats_extract_conf hop_length=256 --feats_extract_conf win_length=null --feats_extract_conf fs=16000 --feats_extract_conf fmin=80 --feats_extract_conf fmax=7600 --feats_extract_conf n_mels=80 --pitch_extract_conf fs=16000 --pitch_extract_conf n_fft=1024 --pitch_extract_conf hop_length=256 --pitch_extract_conf f0max=400 --pitch_extract_conf f0min=80 --energy_extract_conf fs=16000 --energy_extract_conf n_fft=1024 --energy_extract_conf hop_length=256 --energy_extract_conf win_length=null
Vocabulary size: 49
encoder self-attention layer type = self-attention
decoder self-attention layer type = self-attention
Successfully registered stats as buffer.
Vocabulary size: 5000
Initialize encoder.embed.conv.0.bias to zeros
Initialize encoder.embed.conv.2.bias to zeros
Initialize encoder.embed.out.0.bias to zeros
Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.0.norm1.bias to zeros
Initialize encoder.encoders.0.norm2.bias to zeros
Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.1.norm1.bias to zeros
Initialize encoder.encoders.1.norm2.bias to zeros
Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.2.norm1.bias to zeros
Initialize encoder.encoders.2.norm2.bias to zeros
Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.3.norm1.bias to zeros
Initialize encoder.encoders.3.norm2.bias to zeros
Initialize encoder.encoders.4.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.4.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.4.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.4.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.4.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.4.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.4.norm1.bias to zeros
Initialize encoder.encoders.4.norm2.bias to zeros
Initialize encoder.encoders.5.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.5.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.5.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.5.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.5.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.5.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.5.norm1.bias to zeros
Initialize encoder.encoders.5.norm2.bias to zeros
Initialize encoder.encoders.6.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.6.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.6.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.6.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.6.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.6.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.6.norm1.bias to zeros
Initialize encoder.encoders.6.norm2.bias to zeros
Initialize encoder.encoders.7.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.7.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.7.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.7.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.7.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.7.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.7.norm1.bias to zeros
Initialize encoder.encoders.7.norm2.bias to zeros
Initialize encoder.encoders.8.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.8.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.8.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.8.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.8.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.8.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.8.norm1.bias to zeros
Initialize encoder.encoders.8.norm2.bias to zeros
Initialize encoder.encoders.9.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.9.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.9.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.9.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.9.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.9.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.9.norm1.bias to zeros
Initialize encoder.encoders.9.norm2.bias to zeros
Initialize encoder.encoders.10.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.10.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.10.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.10.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.10.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.10.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.10.norm1.bias to zeros
Initialize encoder.encoders.10.norm2.bias to zeros
Initialize encoder.encoders.11.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.11.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.11.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.11.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.11.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.11.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.11.norm1.bias to zeros
Initialize encoder.encoders.11.norm2.bias to zeros
Initialize encoder.encoders.12.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.12.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.12.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.12.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.12.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.12.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.12.norm1.bias to zeros
Initialize encoder.encoders.12.norm2.bias to zeros
Initialize encoder.encoders.13.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.13.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.13.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.13.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.13.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.13.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.13.norm1.bias to zeros
Initialize encoder.encoders.13.norm2.bias to zeros
Initialize encoder.encoders.14.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.14.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.14.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.14.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.14.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.14.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.14.norm1.bias to zeros
Initialize encoder.encoders.14.norm2.bias to zeros
Initialize encoder.encoders.15.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.15.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.15.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.15.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.15.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.15.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.15.norm1.bias to zeros
Initialize encoder.encoders.15.norm2.bias to zeros
Initialize encoder.encoders.16.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.16.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.16.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.16.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.16.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.16.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.16.norm1.bias to zeros
Initialize encoder.encoders.16.norm2.bias to zeros
Initialize encoder.encoders.17.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.17.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.17.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.17.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.17.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.17.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.17.norm1.bias to zeros
Initialize encoder.encoders.17.norm2.bias to zeros
Initialize encoder.after_norm.bias to zeros
Initialize decoder.after_norm.bias to zeros
Initialize decoder.output_layer.bias to zeros
Initialize decoder.decoders.0.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.0.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.0.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.0.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.0.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.0.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.0.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.0.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.0.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.0.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.0.norm1.bias to zeros
Initialize decoder.decoders.0.norm2.bias to zeros
Initialize decoder.decoders.0.norm3.bias to zeros
Initialize decoder.decoders.1.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.1.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.1.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.1.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.1.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.1.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.1.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.1.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.1.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.1.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.1.norm1.bias to zeros
Initialize decoder.decoders.1.norm2.bias to zeros
Initialize decoder.decoders.1.norm3.bias to zeros
Initialize decoder.decoders.2.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.2.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.2.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.2.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.2.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.2.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.2.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.2.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.2.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.2.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.2.norm1.bias to zeros
Initialize decoder.decoders.2.norm2.bias to zeros
Initialize decoder.decoders.2.norm3.bias to zeros
Initialize decoder.decoders.3.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.3.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.3.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.3.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.3.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.3.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.3.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.3.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.3.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.3.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.3.norm1.bias to zeros
Initialize decoder.decoders.3.norm2.bias to zeros
Initialize decoder.decoders.3.norm3.bias to zeros
Initialize decoder.decoders.4.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.4.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.4.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.4.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.4.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.4.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.4.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.4.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.4.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.4.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.4.norm1.bias to zeros
Initialize decoder.decoders.4.norm2.bias to zeros
Initialize decoder.decoders.4.norm3.bias to zeros
Initialize decoder.decoders.5.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.5.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.5.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.5.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.5.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.5.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.5.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.5.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.5.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.5.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.5.norm1.bias to zeros
Initialize decoder.decoders.5.norm2.bias to zeros
Initialize decoder.decoders.5.norm3.bias to zeros
Initialize ctc.ctc_lo.bias to zeros
Vocabulary size: 5000
BatchBeamSearch implementation is selected.
Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): SequentialRNNLM(
      (drop): Dropout(p=0.0, inplace=False)
      (encoder): Embedding(5000, 2048, padding_idx=0)
      (rnn): LSTM(2048, 2048, num_layers=4, batch_first=True)
      (decoder): Linear(in_features=2048, out_features=5000, bias=True)
    )
  )
)
Decoding device=cpu, dtype=float32
Text tokenizer: SentencepiecesTokenizer(model="/home/karimimonsefi.1/miniconda3/envs/espnet_env/lib/python3.9/site-packages/espnet_model_zoo/653d10049fdc264f694f57b49849343e/data/token_list/bpe_unigram5000/bpe.model")
[a100-01] 2023-03-30 09:47:30,782 (vectors:169) INFO: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
Initialize encoder.embed.0.0.convs.0.1.bias to zeros
Initialize encoder.embed.0.0.convs.1.1.bias to zeros
Initialize encoder.embed.0.0.convs.2.1.bias to zeros
Initialize encoder.embed.0.1.bias to zeros
Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.0.norm1.bias to zeros
Initialize encoder.encoders.0.norm2.bias to zeros
Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.1.norm1.bias to zeros
Initialize encoder.encoders.1.norm2.bias to zeros
Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.2.norm1.bias to zeros
Initialize encoder.encoders.2.norm2.bias to zeros
Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.3.norm1.bias to zeros
Initialize encoder.encoders.3.norm2.bias to zeros
Initialize encoder.encoders.4.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.4.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.4.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.4.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.4.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.4.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.4.norm1.bias to zeros
Initialize encoder.encoders.4.norm2.bias to zeros
Initialize encoder.encoders.5.self_attn.linear_q.bias to zeros
Initialize encoder.encoders.5.self_attn.linear_k.bias to zeros
Initialize encoder.encoders.5.self_attn.linear_v.bias to zeros
Initialize encoder.encoders.5.self_attn.linear_out.bias to zeros
Initialize encoder.encoders.5.feed_forward.w_1.bias to zeros
Initialize encoder.encoders.5.feed_forward.w_2.bias to zeros
Initialize encoder.encoders.5.norm1.bias to zeros
Initialize encoder.encoders.5.norm2.bias to zeros
Initialize encoder.after_norm.bias to zeros
Initialize decoder.embed.0.0.prenet.0.0.bias to zeros
Initialize decoder.embed.0.0.prenet.1.0.bias to zeros
Initialize decoder.embed.0.1.bias to zeros
Initialize decoder.decoders.0.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.0.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.0.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.0.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.0.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.0.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.0.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.0.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.0.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.0.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.0.norm1.bias to zeros
Initialize decoder.decoders.0.norm2.bias to zeros
Initialize decoder.decoders.0.norm3.bias to zeros
Initialize decoder.decoders.1.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.1.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.1.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.1.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.1.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.1.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.1.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.1.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.1.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.1.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.1.norm1.bias to zeros
Initialize decoder.decoders.1.norm2.bias to zeros
Initialize decoder.decoders.1.norm3.bias to zeros
Initialize decoder.decoders.2.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.2.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.2.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.2.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.2.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.2.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.2.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.2.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.2.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.2.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.2.norm1.bias to zeros
Initialize decoder.decoders.2.norm2.bias to zeros
Initialize decoder.decoders.2.norm3.bias to zeros
Initialize decoder.decoders.3.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.3.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.3.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.3.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.3.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.3.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.3.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.3.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.3.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.3.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.3.norm1.bias to zeros
Initialize decoder.decoders.3.norm2.bias to zeros
Initialize decoder.decoders.3.norm3.bias to zeros
Initialize decoder.decoders.4.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.4.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.4.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.4.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.4.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.4.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.4.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.4.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.4.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.4.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.4.norm1.bias to zeros
Initialize decoder.decoders.4.norm2.bias to zeros
Initialize decoder.decoders.4.norm3.bias to zeros
Initialize decoder.decoders.5.self_attn.linear_q.bias to zeros
Initialize decoder.decoders.5.self_attn.linear_k.bias to zeros
Initialize decoder.decoders.5.self_attn.linear_v.bias to zeros
Initialize decoder.decoders.5.self_attn.linear_out.bias to zeros
Initialize decoder.decoders.5.src_attn.linear_q.bias to zeros
Initialize decoder.decoders.5.src_attn.linear_k.bias to zeros
Initialize decoder.decoders.5.src_attn.linear_v.bias to zeros
Initialize decoder.decoders.5.src_attn.linear_out.bias to zeros
Initialize decoder.decoders.5.feed_forward.w_1.bias to zeros
Initialize decoder.decoders.5.feed_forward.w_2.bias to zeros
Initialize decoder.decoders.5.norm1.bias to zeros
Initialize decoder.decoders.5.norm2.bias to zeros
Initialize decoder.decoders.5.norm3.bias to zeros
Initialize decoder.after_norm.bias to zeros
Initialize feat_out.bias to zeros
Initialize prob_out.bias to zeros
Initialize postnet.postnet.0.1.bias to zeros
Initialize postnet.postnet.1.1.bias to zeros
Initialize postnet.postnet.2.1.bias to zeros
Initialize postnet.postnet.3.1.bias to zeros
Initialize postnet.postnet.4.1.bias to zeros
pytorch.version=1.13.0+cu117, cuda.available=True, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
Model structure:
ESPnetTTSModel(
  (feats_extract): LogMelFbank(
    (stft): Stft(n_fft=1024, win_length=1024, hop_length=256, center=True, normalized=False, onesided=True)
    (logmel): LogMel(sr=16000, n_fft=1024, n_mels=80, fmin=80, fmax=7600, htk=False)
  )
  (tts): Transformer(
    (encoder): Encoder(
      (embed): Sequential(
        (0): Sequential(
          (0): Encoder(
            (embed): Embedding(49, 512, padding_idx=0)
            (convs): ModuleList(
              (0): Sequential(
                (0): Conv1d(512, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
                (3): Dropout(p=0.5, inplace=False)
              )
              (1): Sequential(
                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
                (3): Dropout(p=0.5, inplace=False)
              )
              (2): Sequential(
                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
                (3): Dropout(p=0.5, inplace=False)
              )
            )
          )
          (1): Linear(in_features=256, out_features=512, bias=True)
        )
        (1): ScaledPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (encoders): MultiSequential(
        (0): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): MultiLayeredConv1d(
            (w_1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
            (w_2): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): MultiLayeredConv1d(
            (w_1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
            (w_2): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): MultiLayeredConv1d(
            (w_1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
            (w_2): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): MultiLayeredConv1d(
            (w_1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
            (w_2): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): MultiLayeredConv1d(
            (w_1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
            (w_2): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): MultiLayeredConv1d(
            (w_1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
            (w_2): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
    )
    (decoder): Decoder(
      (embed): Sequential(
        (0): Sequential(
          (0): Prenet(
            (prenet): ModuleList(
              (0): Sequential(
                (0): Linear(in_features=80, out_features=256, bias=True)
                (1): ReLU()
              )
              (1): Sequential(
                (0): Linear(in_features=256, out_features=256, bias=True)
                (1): ReLU()
              )
            )
          )
          (1): Linear(in_features=256, out_features=512, bias=True)
        )
        (1): ScaledPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=1024, bias=True)
            (w_2): Linear(in_features=1024, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=1024, bias=True)
            (w_2): Linear(in_features=1024, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=1024, bias=True)
            (w_2): Linear(in_features=1024, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=1024, bias=True)
            (w_2): Linear(in_features=1024, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=1024, bias=True)
            (w_2): Linear(in_features=1024, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=1024, bias=True)
            (w_2): Linear(in_features=1024, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
    )
    (feat_out): Linear(in_features=512, out_features=80, bias=True)
    (prob_out): Linear(in_features=512, out_features=1, bias=True)
    (postnet): Postnet(
      (postnet): ModuleList(
        (0): Sequential(
          (0): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Tanh()
          (3): Dropout(p=0.5, inplace=False)
        )
        (1): Sequential(
          (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Tanh()
          (3): Dropout(p=0.5, inplace=False)
        )
        (2): Sequential(
          (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Tanh()
          (3): Dropout(p=0.5, inplace=False)
        )
        (3): Sequential(
          (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Tanh()
          (3): Dropout(p=0.5, inplace=False)
        )
        (4): Sequential(
          (0): Conv1d(256, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
          (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (criterion): TransformerLossWithASR(
      (l1_criterion): L1Loss()
      (mse_criterion): MSELoss()
      (bce_criterion): BCEWithLogitsLoss()
      (text_loss): TextCosineEmbeddingLoss()
    )
    (attn_criterion): GuidedMultiHeadAttentionLoss()
  )
)

Model summary:
    Class Name: ESPnetTTSModel
    Total Number of model parameters: 34.46 M
    Number of trainable parameters: 34.46 M (100.0%)
    Size: 137.86 MB
    Type: torch.float32
Optimizer:
Adadelta (
Parameter Group 0
    eps: 1e-06
    foreach: None
    lr: 1.0
    maximize: False
    rho: 0.9
    weight_decay: 0
)
Scheduler: None
Saving the configuration in exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/stats.10/config.yaml
Namespace(config='conf/train_transformer.yaml', print_config=False, log_level='INFO', dry_run=False, iterator_type='sequence', output_dir='exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/stats.10', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, collect_stats=True, write_collected_feats=False, max_epoch=2, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[('train', 'loss', 'min'), ('valid', 'loss', 'min'), ('train', 'acc', 'max'), ('valid', 'acc', 'max')], keep_nbest_models=[10], nbest_averaging_interval=0, grad_clip=5.0, grad_clip_type=2.0, grad_noise=False, accum_grad=1, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=1, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=10, batch_size=20, valid_batch_size=None, batch_bins=1000000, valid_batch_bins=None, train_shape_file=['exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/train.10.scp'], valid_shape_file=['exp/tts_stats_raw_phn_tacotron_g2p_en/logdir/valid.10.scp'], batch_type='folded', valid_batch_type=None, fold_length=[], sort_in_batch='descending', sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], train_data_path_and_name_and_type=[('dump/raw/train_nodev/text', 'text', 'text'), ('dump/raw/train_nodev/wav.scp', 'speech', 'sound')], valid_data_path_and_name_and_type=[('dump/raw/train_dev/text', 'text', 'text'), ('dump/raw/train_dev/wav.scp', 'speech', 'sound')], allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adadelta', optim_conf={}, scheduler=None, scheduler_conf={}, token_list=['<blank>', '<unk>', ' ', 'IY1', 'T', 'N', 'S', 'EH1', 'R', 'AY1', 'F', 'AH0', 'EY1', 'V', 'AH1', 'UW1', 'W', 'K', 'IH1', 'IY0', 'B', 'D', 'AA1', 'OW1', 'L', 'Y', 'TH', 'AO1', 'JH', 'P', 'CH', 'M', 'ER0', 'Z', 'UW0', 'IH0', 'AW2', 'ER1', 'OW0', 'HH', 'G', 'AW1', 'AE1', 'EH2', 'EH0', 'AA0', 'UW2', 'EY0', '<sos/eos>'], odim=None, model_conf={}, use_preprocessor=True, token_type='phn', bpemodel=None, non_linguistic_symbols=None, cleaner='tacotron', g2p='g2p_en', feats_extract='fbank', feats_extract_conf={'n_fft': 1024, 'hop_length': 256, 'win_length': None, 'fs': 16000, 'fmin': 80, 'fmax': 7600, 'n_mels': 80}, normalize=None, normalize_conf={}, tts='transformer', tts_conf={'loss_type': 'ASR'}, pitch_extract=None, pitch_extract_conf={'fs': 16000, 'n_fft': 1024, 'hop_length': 256, 'f0max': 400, 'f0min': 80}, pitch_normalize=None, pitch_normalize_conf={}, energy_extract=None, energy_extract_conf={'fs': 16000, 'n_fft': 1024, 'hop_length': 256, 'win_length': None}, energy_normalize=None, energy_normalize_conf={}, required=['output_dir', 'token_list'], version='202301', distributed=False)
Niter: 1
Niter: 2
Niter: 3
Niter: 1
# Accounting: time=48 threads=1
# Ended (code 0) at Thu Mar 30 09:47:41 CDT 2023, elapsed time 48 seconds
